{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"2ef9fed5ce864566ad2bae5aed887fb0","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Learning with the newspapers ðŸ“°"]},{"cell_type":"markdown","metadata":{"cell_id":"4afa8ac51f5d4032aed55b4121178060","deepnote_cell_type":"markdown"},"source":["## Introduction\n","\n","The primary objective of this data science project is to build a text classification model capable of  predicting the newspaper that published them. Being able to attribute articles to their respective newspapers can have practical applications in content analysis and media scoring.\n","\n","In this project, we will leverage natural language processing techniques and machine learning algorithms to develop an accurate language and newspaper classification model. By the end of this notebook, we aim to create two models that can help us predict the target class. We goal of this notebook is to show the step necessary to create a machine learning pipeline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3a82386a19b3436490f0ad67321c8d61","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":14,"execution_start":1690569635853,"source_hash":null},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9f6dfdd34d02485aa064ebf7c6570301","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2608,"execution_start":1690569637213,"source_hash":null},"outputs":[],"source":["import re\n","import nltk \n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords','./cache/')\n","nltk.download('wordnet','./cache/')\n","nltk.download('omw-1.4','./cache/')\n","nltk.data.path.append(\"./cache/\")\n","print(stopwords.fileids())\n","\n","# Downloading the stopwords for English in order to simplify the text\n","stopwords_both = set(stopwords.words('english'))"]},{"cell_type":"markdown","metadata":{"cell_id":"0342c54ce1824060bad9fad99f3e6dab","deepnote_cell_type":"markdown"},"source":["## Data Collection and Preprocessing\n","\n","To achieve our goal, we have a labeled dataset that includes news articles written in both English and Japanese, along with the information about the newspaper source for each article. The data will be collected from reliable news websites and public datasets, with the necessary metadata for each article.\n","\n","Before training the classification model, we need to preprocess the textual data. This preprocessing step involves removing irrelevant information such as HTML tags, special characters, and punctuation. Additionally, we will convert all text to lowercase to ensure consistent and case-insensitive feature extraction."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"224940e65aae4c38abaea419da74824f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3244,"execution_start":1690569639814,"source_hash":null},"outputs":[],"source":["def collect_data():\n","    english_news_df = pd.read_csv('./archive/english_news.csv',delimiter='\\t')\n","    japanese_news_df = pd.read_csv('./archive/japanese_news.csv',delimiter='\\t')\n","    \n","    # Add a 'language' column to each DataFrame to indicate the language\n","    english_news_df['language'] = 'English'\n","    japanese_news_df['language'] = 'Japanese'\n","    \n","    # Concatenate the two DataFrames into one\n","    combined_df = pd.concat([english_news_df, japanese_news_df], ignore_index=True).reset_index()\n","    \n","    return combined_df\n","\n","# Call the data collection function to get the DataFrame\n","news_df = collect_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"57f752a6a96c42febc3b1b1f1853aafe","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":30181,"execution_start":1690556573165,"source_hash":null},"outputs":[],"source":["import string as sss\n","def clean_to_list(text):\n","    text_data = re.sub('[^a-zA-Z]', ' ', text)\n","    text_data = text_data.lower()\n","    text_data = text_data.split()\n","    wl = WordNetLemmatizer()\n","    text_data = [wl.lemmatize(word) for word in text_data if not word in stopwords_both]\n","    return ' '.join(text_data)\n","\n","\n","def preprocess_text(text):\n","    if pd.isna(text):\n","        return \"\"\n","    # Convert text to lowercase\n","    text = text.lower()\n","    \n","    # Remove HTML tags (if any)\n","    text = re.sub(r'<.*?>', '', text)\n","    \n","    # Remove special characters and punctuation\n","    text = text.translate(str.maketrans('', '', sss.punctuation))\n","    \n","    #Clean via nltk: remove stopwords and lemmatize\n","    return clean_to_list(text)\n","\n","\n","# Apply preprocessing to the 'article_text' column in the DataFrame\n","news_df.loc[:,'preprocessed_text'] = news_df['text'].apply(preprocess_text)\n","news_df.loc[:,'preprocessed_title'] = news_df['title'].apply(preprocess_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"04e798eafbf74ec49a871ad78dd0bcf8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1502,"execution_start":1690556603320,"source_hash":null},"outputs":[],"source":["news_df.describe(include='all')"]},{"cell_type":"markdown","metadata":{"cell_id":"837083b53b59489a95cabdd701755b4b","deepnote_cell_type":"markdown"},"source":["## Data Exploration and Visualization\n","\n","Before diving into model building, it is essential to gain insights into the dataset. We will visualize the distribution of news articles from different newspapers and their corresponding languages. Understanding the class balance will help us identify potential biases and address any data-related challenges.\n","\n","Furthermore, we will analyze the length of the articles and the distribution of words to gather valuable information that may impact our classification model's performance.\n","\n","Let's begin by collecting and preprocessing the data, followed by exploratory data analysis to prepare our data for the language and newspaper classification task."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"199bc4513e454c89856bed39ddf0fe17","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":3067,"execution_start":1690556604401,"source_hash":null},"outputs":[],"source":["# Function to analyze the article length and word distribution\n","def analyze_article_length(data_df):\n","    data_df['article_length'] = data_df['preprocessed_text'].apply(lambda x: len(x.split()))\n","    \n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(data_df['article_length'], bins=50, kde=True, color='skyblue')\n","    plt.title('Distribution of Article Length')\n","    plt.xlabel('Number of Words')\n","    plt.ylabel('Frequency')\n","    plt.show()\n","\n","analyze_article_length(news_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"61c83c2e4f6242cc828aeb6769b6d463","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":856,"execution_start":1690556607409,"source_hash":null},"outputs":[],"source":["def plot_newspaper_distribution(data_df):\n","    plt.figure(figsize=(10, 6))\n","    sns.countplot(x='source', data=data_df, palette='viridis')\n","    plt.title('Distribution of News Articles by Newspaper')\n","    plt.xlabel('Newspaper')\n","    plt.ylabel('Number of Articles')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","\n","plot_newspaper_distribution(news_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b0a32e1f3c214c9496dc5482dc565f29","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":889,"execution_start":1690556608100,"source_hash":null},"outputs":[],"source":["# Function to visualize the number of articles over time\n","def plot_articles_over_time(data_df):\n","    # Assuming the date of the news articles is stored in a column named 'date'\n","    data_df['date'] = pd.to_datetime(data_df['date'])\n","    data_df = data_df.sort_values(by='date').reset_index(drop=True)\n","    \n","    plt.figure(figsize=(10, 6))\n","    sns.lineplot(x='date', y=data_df.index, data=data_df, estimator=None, lw=1, color='blue')\n","    plt.title('Number of Articles Over Time')\n","    plt.xlabel('Date')\n","    plt.ylabel('Number of Articles')\n","    plt.show()\n","\n","plot_articles_over_time(news_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d57a895115d2437ea68dde18e0a929a9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":68610,"execution_start":1690556608841,"source_hash":null},"outputs":[],"source":["def plot_avg_article_length_over_time(data_df):\n","    data_df['date'] = pd.to_datetime(data_df['date'])\n","    data_df['article_length'] = data_df['preprocessed_text'].apply(lambda x: len(x.split()))\n","    data_df = data_df.sort_values(by='date').reset_index(drop=True)\n","    \n","    plt.figure(figsize=(10, 6))\n","    sns.lineplot(x='date', y='article_length', data=data_df, estimator='mean', lw=2, color='orange')\n","    plt.title('Average Article Length Over Time')\n","    plt.xlabel('Date')\n","    plt.ylabel('Average Article Length (Words)')\n","    plt.show()\n","\n","plot_avg_article_length_over_time(news_df)"]},{"cell_type":"markdown","metadata":{"cell_id":"84af354e61f64bbd982b04a6d3291c35","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Machine Learning"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we will perform a multi-class classification task using XGBoost, a powerful gradient boosting algorithm, and leverage Word2Vec to convert news articles into meaningful numerical representations suitable for input to XGBoost.\n","\n","### Leveraging Word2Vec for XGBoost Input\n","\n","Once we obtain the document-level vectors using Word2Vec, we can use them as features for training the XGBoost model. These numerical representations will provide valuable information to the model, enabling it to make accurate predictions on unseen news articles.\n","\n","By combining the power of XGBoost and Word2Vec, we aim to create a robust classification system capable of distinguishing the publishers of English and Japanese news articles effectively. The integration of these two techniques allows us to handle the complexities of multiclass classification while leveraging the semantic context of the news articles for improved predictive performance."]},{"cell_type":"markdown","metadata":{},"source":["### Word2Vec for Text Embeddings\n","\n","As news articles are typically represented as textual data, we need to convert them into numerical vectors before feeding them into the XGBoost model. Word2Vec, a popular word embedding technique, will help us accomplish this task.\n","\n","Word2Vec represents words in a dense vector space where semantically similar words are located close together. By applying Word2Vec to the news articles' text, we will create meaningful embeddings that capture the contextual information of each article."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6495a70f70744bee81fe380ad08e2ad2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":488,"execution_start":1690556681538,"source_hash":null},"outputs":[],"source":["import pandas as pd\n","from gensim.models import Word2Vec\n","\n","def train_word2vec_model(sentences, vector_size=32, window=6, min_count=2, workers=4):\n","    # Train the Word2Vec model creating 32 feature for each word\n","    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n","    return model\n","\n","def get_document_vectors(df, model):\n","    # Prepare the text data\n","    sentences = [text.split() for text in df['preprocessed_text']]\n","\n","    # Obtain document-level vectors for all documents in the DataFrame\n","    document_vectors = []\n","    for document in sentences:\n","        doc_vector = []\n","        for word in document:\n","            try:\n","                # Try to get the word vector for each word in the document\n","                word_vector = model.wv[word]\n","                doc_vector.append(word_vector)\n","            except KeyError:\n","                # If the word is not present in the vocabulary, skip it\n","                continue\n","\n","        # Calculate the document-level vector by averaging word vectors\n","        if doc_vector:\n","            doc_vector = sum(doc_vector) / len(doc_vector)\n","        else:\n","            # Handle the case when the document is empty or all words are not present in the vocabulary\n","            doc_vector = [0.0] * model.vector_size\n","\n","        document_vectors.append(doc_vector)\n","\n","    # Convert the list of document vectors into a DataFrame\n","    document_vectors_df = pd.DataFrame(document_vectors)\n","\n","    return document_vectors_df\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocessing\n","Let's do some feature engineering and prepare the dataframe for the ML model. \n","We make sure the data is the best suited for the downstream process. Removing outliers, scaling numbers properly and one-hot encoding categorical values. "]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ab819be8141946f58cd86348b02584ec","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":10,"sortBy":[]},"deepnote_to_be_reexecuted":true,"execution_millis":698,"execution_start":1690556682022,"source_hash":null},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","features_df = news_df.drop(['text','title','preprocessed_title','index'], axis=1).copy(deep=True)\n","\n","features_df['article_length'] = features_df['preprocessed_text'].apply(lambda x: len(x.split()))\n","\n","features_df['date'] = pd.to_datetime(features_df['date'])\n","# Set the fixed reference point as January 1, 2000\n","fixed_reference_point = pd.Timestamp('2000-01-01')\n","# Calculate the time elapsed since the fixed reference point\n","features_df['time_elapsed'] = (features_df['date'] - fixed_reference_point).dt.total_seconds()\n","# Drop the original datetime column as we have converted it to a numeric representation\n","features_df.drop('date', axis=1, inplace=True)\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","# Fit and transform the selected column\n","features_df['article_length'] = scaler.fit_transform(features_df[['article_length']])\n","features_df['time_elapsed'] = scaler.fit_transform(features_df[['time_elapsed']])\n","\n","features_df['language'] = features_df['language'].map(lambda x: 1 if x == 'English' else 0)\n","\n","## Perform one-hot encoding on the 'category' column\n","#one_hot_encoded = pd.get_dummies(features_df['author'])\n","## Concatenate the one-hot encoded columns with the original DataFrame\n","#features_df = pd.concat([features_df, one_hot_encoded], axis=1)\n","\n","features_df.drop('author', axis=1, inplace=True)\n","features_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Extract the class labels from the DataFrame\n","y_labels = features_df['source']\n","# Initialize the LabelEncoder\n","label_encoder = LabelEncoder()\n","# Fit the LabelEncoder on the class labels and transform them to numeric representations\n","features_df['target'] = label_encoder.fit_transform(y_labels)\n","features_df['target'] = pd.to_numeric(features_df['target'])\n","# Obtain the mapping of numeric labels to original class labels\n","class_label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_pairplot(dataframe):\n","\n","    # Plot the pairplot\n","    sns.pairplot(dataframe, hue='target', diag_kind='hist')\n","\n","    # Show the plot\n","    plt.figure(figsize=(10, 6))\n","    plt.show()\n","\n","plot_pairplot(features_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"allow_embed":false,"cell_id":"d3cba65e08b448088e5efc96f675111e","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":10,"sortBy":[]},"deepnote_to_be_reexecuted":true,"execution_millis":32731,"execution_start":1690556682827,"source_hash":null},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","yyy = features_df['target'].copy(deep=True)\n","features_df = features_df.drop(['source','target'],axis=1)\n","X_train, X_test, y_train, y_test = train_test_split(features_df.copy(deep=True), yyy, test_size=0.25, random_state=42)\n","# Convert numpy arrays back to DataFrames\n","X_train_df = pd.DataFrame(X_train, columns=features_df.columns)\n","X_test_df = pd.DataFrame(X_test, columns=features_df.columns)\n","\n","# Training the Word2Vec model only with the trainig data\n","train_sentences = [text.split() for text in X_train_df['preprocessed_text']]\n","word2vec_model = train_word2vec_model(train_sentences)\n","\n","# Obtain document vectors for train DataFrame using the trained Word2Vec model\n","train_document_vectors_df = get_document_vectors(X_train_df, word2vec_model)\n","# Obtain document vectors for test DataFrame using the same trained Word2Vec model\n","test_document_vectors_df = get_document_vectors(X_test_df, word2vec_model)\n","\n","\n","X_train_df = X_train_df.drop('preprocessed_text',axis=1)\n","X_test_df = X_test_df.drop('preprocessed_text',axis=1)\n","\n","X_train_pre = pd.concat([X_train_df, train_document_vectors_df], axis=1)\n","X_test_pre = pd.concat([X_test_df, test_document_vectors_df], axis=1)\n","\n","# Rename the columns to make them more meaningful\n","num_dimensions = len(train_document_vectors_df.columns)\n","column_names = [f'vector_{i}' for i in range(num_dimensions)]\n","X_train_pre.columns = X_train_df.columns.tolist() + column_names\n","X_test_pre.columns = X_test_df.columns.tolist() + column_names\n","\n","X_test_pre.dropna(inplace=True)\n","X_train_pre.dropna(inplace=True)\n","\n","# Now the DataFrame contains the original data along with document-level vectors in each row\n","X_train_pre"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a44f9fb4a25c4545a2174d2c22152a5c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":13,"execution_start":1690556715458,"source_hash":null},"outputs":[],"source":["# Back to numpy arrays\n","X_train = X_train_df.values\n","X_test = X_test_df.values"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline\n","The starting point is always a baseline stupid model to beat. Here we just guess the class based on the frequency we saw in the dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ea15e14d201b4a069a20dcca9ad02f8f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":19,"execution_start":1690556719354,"source_hash":null},"outputs":[],"source":["import numpy as np\n","\n","# Calculate the number of classes\n","num_classes = np.max(y_train) + 1\n","\n","# Calculate the proportion of each class in the training set\n","class_proportions = np.bincount(y_train) / len(y_train)\n","\n","# Generate random predictions for the test set based on the class proportions\n","random_predictions = np.random.choice(num_classes, size=len(y_test), p=class_proportions)\n","\n","# Calculate the accuracy of the random baseline model\n","accuracy_random_baseline = np.mean(y_test == random_predictions)\n","print(\"Accuracy of the random baseline:\", accuracy_random_baseline)"]},{"cell_type":"markdown","metadata":{},"source":["### XGBoost for Multi-Class Classification\n","\n","XGBoost (Extreme Gradient Boosting) is a widely used machine learning algorithm known for its effectiveness in handling complex classification problems. It can handle both binary and multi-class classification tasks efficiently while maintaining high predictive accuracy.\n","\n","We will utilize XGBoost to classify news articles into one of multiple classes based on their publishers. The goal is to predict the newspaper that published each article, which falls into one of several categories.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d3352e3d0566472b895b6429e193417c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":115537,"execution_start":1690556719368,"source_hash":null},"outputs":[],"source":["import pandas as pd\n","import xgboost as xgb\n","from sklearn.metrics import accuracy_score\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","# Initialize the XGBoost classifier\n","clf = xgb.XGBClassifier(\n","    objective='multi:softmax',  # Specify the objective function for multi-class classification\n","    num_class=len(class_label_mapping.keys()),\n","    n_estimators=100,\n","    max_depth=3,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42\n",")\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","\n","# Make predictions on the test data\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the classifier's performance accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"cell_id":"5c77235af23a46ce9149308bd5110863","deepnote_cell_type":"text-cell-h1","formattedRanges":[],"is_collapsed":false},"source":["# Deep Learning"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we explore the implementation of deep learning techniques, specifically utilizing BERT (Bidirectional Encoder Representations from Transformers) for the task of classifying news articles based on their publishers. BERT, a state-of-the-art language model, is chosen for its ability to capture complex contextual information from text data, making it highly effective for natural language processing tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e49b78a7a779487b923a0145febf2b15","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":756,"execution_start":1690569643189,"source_hash":null},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","#Prepare Dataset\n","ml_df = news_df[['source','text','language']].copy(deep=True).dropna()\n","ml_df = ml_df.sample(frac=0.03, random_state=42).reset_index(drop=True)\n","\n","# Initialize the LabelEncoder\n","label_encoder = LabelEncoder()\n","# Fit the LabelEncoder on the class labels and transform them to numeric representations\n","ml_df['target'] = label_encoder.fit_transform(ml_df['source'])\n","ml_df['target'] = pd.to_numeric(ml_df['target'])\n","ml_df = ml_df.drop('source',axis=1)\n","num_classes = len(ml_df.loc[:,'target'].unique())\n","\n","ml_df.describe(include='all')"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing for BERT\n","\n","Preparing the data for BERT involves several essential preprocessing steps to ensure it functions effectively. The main preprocessing steps are as follows:\n","\n","**Tokenization**\n","\n","BERT requires tokenization, which involves breaking down the news articles into individual tokens (words or subwords) and converting them into their corresponding numerical IDs. Tokenization ensures that the text can be effectively processed by BERT's neural network architecture.\n","\n","**Padding and Truncation**\n","\n","BERT expects fixed-length input sequences, so we need to handle varying text lengths in the news articles. Padding is applied to ensure all sequences have the same length by adding special tokens, while truncation is used to reduce longer sequences to the desired length.\n","\n","**Attention Masks**\n","\n","Since BERT operates on fixed-length sequences, attention masks are created to indicate which tokens are actual words and which are padding tokens. Attention masks help the model focus only on the relevant parts of the input while ignoring the padding tokens.\n","\n","**Language-Specific Token Embeddings**\n","\n","As we have both English and Japanese news articles, we must use separate tokenizers and embeddings for each language. We utilize language-specific token embeddings to represent each token in its corresponding language."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e0b7a95d706e454a9bd398ad1d0a76b2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":24056,"execution_start":1690569648340,"source_hash":null},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","\n","# Load pre-trained BERT models and tokenizers for English and Japanese\n","english_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n","english_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","japanese_model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-v3', num_labels=num_classes)\n","japanese_tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n","\n","# Define the optimizer and learning rate for both models\n","optimizer_en = AdamW(english_model.parameters(), lr=2e-5)\n","optimizer_ja = AdamW(japanese_model.parameters(), lr=2e-5)\n","\n","# Tokenize and preprocess the text data with the appropriate tokenizer based on the language\n","def preprocess_text(text_data, language, max_length, tokenizer_en, tokenizer_ja):\n","    input_ids = []\n","    attention_masks = []\n","\n","    for text, lang in zip(text_data, language):\n","        if lang == 'English':  # English\n","            encoded = tokenizer_en.encode_plus(text, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n","        elif lang == 'Japanese':  # Japanese\n","            encoded = tokenizer_ja.encode_plus(text, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n","        else:\n","            raise ValueError(\"Unsupported language:\", lang)\n","\n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","\n","    return input_ids, attention_masks\n","\n","# Prepare data for PyTorch DataLoader\n","def create_data_loader(input_ids, attention_masks, labels, batch_size):\n","    input_ids = torch.stack(input_ids)\n","    attention_masks = torch.stack(attention_masks)\n","    labels = torch.tensor(labels)\n","\n","    data = TensorDataset(input_ids, attention_masks, labels)\n","    data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n","\n","    return data_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3467c22d30b34b7bb713d9cacad42b47","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":24381,"execution_start":1690569672249,"source_hash":null},"outputs":[],"source":["# Example usage with DataFrame split\n","max_length = 128\n","batch_size = 64\n","\n","# Assuming you have already loaded your DataFrame 'df'\n","# Split the DataFrame into train, validation, and test sets\n","train_data, temp_data = train_test_split(ml_df, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","\n","# Preprocess text for train, validation, and test data\n","train_input_ids, train_attention_masks = preprocess_text(train_data['text'], train_data['language'], max_length, english_tokenizer, japanese_tokenizer)\n","val_input_ids, val_attention_masks = preprocess_text(val_data['text'], val_data['language'], max_length, english_tokenizer, japanese_tokenizer)\n","test_input_ids, test_attention_masks = preprocess_text(test_data['text'], test_data['language'], max_length, english_tokenizer, japanese_tokenizer)\n","\n","print(\"preprocessing DONE\")"]},{"cell_type":"markdown","metadata":{},"source":["### Leveraging BERT for Classification\n","\n","After preprocessing the news articles and obtaining the BERT token embeddings, we can fine-tune the pre-trained BERT model using our labeled data. Fine-tuning allows BERT to adapt to our specific classification task and optimize its parameters based on the given dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6b4fc185ffc94c65a8f1fe4311a8f1e0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":78,"execution_start":1690569696624,"source_hash":null},"outputs":[],"source":["# Convert input_ids and attention_masks to PyTorch tensors\n","train_input_ids = torch.stack(train_input_ids)\n","train_attention_masks = torch.stack(train_attention_masks)\n","\n","val_input_ids = torch.stack(val_input_ids)\n","val_attention_masks = torch.stack(val_attention_masks)\n","\n","test_input_ids = torch.stack(test_input_ids)\n","test_attention_masks = torch.stack(test_attention_masks)\n","\n","# Convert target labels to a 1D array (list or pandas Series)\n","train_labels = train_data['target'].values\n","val_labels = val_data['target'].values\n","test_labels = test_data['target'].values\n","\n","# Create data loaders for train, validation, and test sets\n","train_data_loader = DataLoader(TensorDataset(train_input_ids, train_attention_masks, torch.tensor(train_labels)), batch_size=batch_size, shuffle=True)\n","val_data_loader = DataLoader(TensorDataset(val_input_ids, val_attention_masks, torch.tensor(val_labels)), batch_size=batch_size, shuffle=False)\n","test_data_loader = DataLoader(TensorDataset(test_input_ids, test_attention_masks, torch.tensor(test_labels)), batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9bf02a742bac4df88ff6bf4051365efe","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":432,"execution_start":1690569741030,"source_hash":null},"outputs":[],"source":["# Fine-tuning for English texts\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","english_model.to(device)\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    english_model.train()\n","    for batch in train_data_loader:\n","        input_ids, attention_masks, labels = batch\n","        input_ids = input_ids.squeeze()\n","        attention_masks = attention_masks.squeeze()\n","        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n","\n","        # Identify the language of each text in the batch\n","        languages = train_data.iloc[input_ids[:, 0]]['language']\n","        # Mask to extract English inputs\n","        mask = torch.tensor((languages == 'English').to_numpy())\n","        assert mask.dim() == 1 and mask.size(0) == input_ids.size(0), \"Mask size does not match input_ids size\"\n","        # Check if there are any English samples in the batch before proceeding\n","        if mask.any():\n","            input_ids_en = input_ids[mask]\n","            attention_masks_en = attention_masks[mask]\n","            labels_en = labels[mask]\n","\n","            outputs = english_model(input_ids_en, attention_mask=attention_masks_en, labels=labels_en)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer_en.step()\n","            optimizer_en.zero_grad()\n","\n","    # Validation after each epoch\n","    english_model.eval()\n","    with torch.no_grad():\n","        total_val_loss = 0.0\n","        correct_predictions = 0\n","\n","        for batch in val_data_loader:\n","            input_ids, attention_masks, labels = batch\n","            input_ids = input_ids.squeeze()\n","            attention_masks = attention_masks.squeeze()\n","            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n","\n","            # Identify the language of each text in the batch\n","            languages = val_data.iloc[input_ids[:, 0]]['language']\n","            # Mask to extract English inputs\n","            mask = torch.tensor((languages == 'English').to_numpy())\n","            assert mask.dim() == 1 and mask.size(0) == input_ids.size(0), \"Mask size does not match input_ids size\"\n","            # Check if there are any English samples in the batch before proceeding\n","            if mask.any():\n","                input_ids_en = input_ids[mask]\n","                attention_masks_en = attention_masks[mask]\n","                labels_en = labels[mask]\n","\n","                outputs = english_model(input_ids_en, attention_mask=attention_masks_en, labels=labels_en)\n","                val_loss = outputs.loss\n","                total_val_loss += val_loss.item()\n","\n","                logits = outputs.logits\n","                predictions = torch.argmax(logits, dim=1)\n","                correct_predictions += (predictions == labels_en).sum().item()\n","\n","        val_accuracy = correct_predictions / len(val_data_loader.dataset)\n","        avg_val_loss = total_val_loss / len(val_data_loader)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ce8b8d8a6a764c529654e636c029ad78","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":null},"outputs":[],"source":["# Fine-tuning for Japanese texts\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","japanese_model.to(device)\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    japanese_model.train()\n","    for batch in train_data_loader:\n","        input_ids, attention_masks, labels = batch\n","        input_ids = input_ids.squeeze()\n","        attention_masks = attention_masks.squeeze()\n","        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n","\n","        # Identify the language of each text in the batch\n","        languages = train_data.iloc[input_ids[:, 0]]['language']\n","        # Mask to extract English inputs\n","        mask = torch.tensor((languages == 'Japanese').to_numpy())\n","        assert mask.dim() == 1 and mask.size(0) == input_ids.size(0), \"Mask size does not match input_ids size\"\n","        input_ids_ja = input_ids[mask]\n","        attention_masks_ja = attention_masks[mask]\n","        labels_ja = labels[mask]\n","\n","        outputs = japanese_model(input_ids_ja, attention_mask=attention_masks_ja, labels=labels_ja)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer_en.step()\n","        optimizer_en.zero_grad()\n","\n","    # Validation after each epoch\n","    japanese_model.eval()\n","    with torch.no_grad():\n","        total_val_loss = 0.0\n","        correct_predictions = 0\n","\n","        for batch in val_data_loader:\n","            input_ids, attention_masks, labels = batch\n","            input_ids = input_ids.squeeze()\n","            attention_masks = attention_masks.squeeze()\n","            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n","\n","            # Identify the language of each text in the batch\n","            languages = val_data.iloc[input_ids[:, 0]]['language']\n","            # Mask to extract Japanese inputs\n","            mask = torch.tensor((languages == 'Japanese').to_numpy())\n","            assert mask.dim() == 1 and mask.size(0) == input_ids.size(0), \"Mask size does not match input_ids size\"\n","            input_ids_ja = input_ids[mask]\n","            attention_masks_ja = attention_masks[mask]\n","            labels_ja = labels[mask]\n","\n","            outputs = japanese_model(input_ids_ja, attention_mask=attention_masks_ja, labels=labels_ja)\n","            val_loss = outputs.loss\n","            total_val_loss += val_loss.item()\n","\n","            logits = outputs.logits\n","            predictions = torch.argmax(logits, dim=1)\n","            correct_predictions += (predictions == labels_ja).sum().item()\n","\n","        val_accuracy = correct_predictions / len(val_data_loader.dataset)\n","        avg_val_loss = total_val_loss / len(val_data_loader)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7a4363a908af4ee4904c9df87361f63f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":null},"outputs":[],"source":["# Evaluation on the test set after training\n","english_model.eval()\n","japanese_model.eval()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with torch.no_grad():\n","    total_test_loss_en = 0.0\n","    total_test_loss_ja = 0.0\n","    correct_predictions_en = 0\n","    correct_predictions_ja = 0\n","\n","    for batch in test_data_loader:\n","        input_ids, attention_masks, labels = batch\n","        input_ids = input_ids.squeeze()\n","        attention_masks = attention_masks.squeeze()\n","        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n","\n","\n","        # Identify the language of each text in the batch\n","        languages = train_data.iloc[input_ids[:, 0]]['language']\n","        # Mask to extract English inputs\n","        mask_ja = torch.tensor((languages == 'Japanese').to_numpy())\n","        mask_en = torch.tensor((languages == 'English').to_numpy())\n","\n","        # Separate the input data into English and Japanese texts\n","        input_ids_en = input_ids[mask_en]\n","        attention_masks_en = attention_masks[mask_en]\n","        labels_en = labels[mask_en]\n","\n","        input_ids_ja = input_ids[mask_ja]\n","        attention_masks_ja = attention_masks[mask_ja]\n","        labels_ja = labels[mask_ja]\n","\n","        # Test English texts using english_model\n","        if len(input_ids_en) > 0:\n","            outputs_en = english_model(input_ids_en, attention_mask=attention_masks_en, labels=labels_en)\n","            test_loss_en = outputs_en.loss\n","            total_test_loss_en += test_loss_en.item()\n","\n","            logits_en = outputs_en.logits\n","            predictions_en = torch.argmax(logits_en, dim=1)\n","            correct_predictions_en += (predictions_en == labels_en).sum().item()\n","\n","        # Test Japanese texts using japanese_model\n","        if len(input_ids_ja) > 0:\n","            outputs_ja = japanese_model(input_ids_ja, attention_mask=attention_masks_ja, labels=labels_ja)\n","            test_loss_ja = outputs_ja.loss\n","            total_test_loss_ja += test_loss_ja.item()\n","\n","            logits_ja = outputs_ja.logits\n","            predictions_ja = torch.argmax(logits_ja, dim=1)\n","            correct_predictions_ja += (predictions_ja == labels_ja).sum().item()\n","\n","    test_accuracy_en = correct_predictions_en / len(test_data[test_data['language'] == 'English'])\n","    test_accuracy_ja = correct_predictions_ja / len(test_data[test_data['language'] == 'Japanese'])\n","\n","    avg_test_loss_en = total_test_loss_en / len(test_data[test_data['language'] == 'English'])\n","    avg_test_loss_ja = total_test_loss_ja / len(test_data[test_data['language'] == 'Japanese'])\n","\n","    print(f\"English Test Loss: {avg_test_loss_en:.4f}, English Test Accuracy: {test_accuracy_en:.4f}\")\n","    print(f\"Japanese Test Loss: {avg_test_loss_ja:.4f}, Japanese Test Accuracy: {test_accuracy_ja:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["That's it folks!"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"5ae418752b8f43ddb0414b2bf4f98905","kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
